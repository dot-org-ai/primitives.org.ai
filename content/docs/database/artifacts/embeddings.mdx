---
title: Embeddings
description: Vector embeddings for semantic search
---

# Embeddings

Vector embeddings power semantic search and similarity matching.

## Automatic Embeddings

Embeddings are generated automatically when you create content:

```typescript
const post = await db.Post.create({
  title: 'Getting Started with TypeScript',
  content: 'TypeScript is a typed superset of JavaScript...',
})

// Embedding auto-generated
// Now searchable via db.search()
```

## Accessing Embeddings

```typescript
const embedding = await db.artifact.get(post.$id, 'embedding')

embedding.vectors     // Float32Array
embedding.model       // 'google/gemini-embedding-001'
embedding.dimensions  // 3072
embedding.sourceHash  // hash of source content
```

## Embedding Models

Configure the embedding model:

```typescript
// Database default
const db = DB({
  model: 'google/gemini-embedding-001',
  dimensions: 3072,
})

// Per-type override
const db = DB({
  model: 'google/gemini-embedding-001',
  dimensions: 3072,

  types: {
    Post: {
      // Use different model for posts
      model: 'baai/bge-m3',
      dimensions: 1024,
    },
  },
})
```

### Available Models

| Model | Dimensions | Best For |
|-------|------------|----------|
| `google/gemini-embedding-001` | 3072 | General (default) |
| `baai/bge-m3` | 1024 | Multilingual |
| `openai/text-embedding-3-large` | 3072 | OpenAI ecosystem |
| `openai/text-embedding-3-small` | 512 | Cost-sensitive |

## Manual Embedding

Generate embeddings directly:

```typescript
// Single text
const embedding = await db.embed('Hello world')
// Float32Array [0.1, -0.2, 0.3, ...]

// Multiple texts
const embeddings = await db.embed(['doc1', 'doc2', 'doc3'])
// Float32Array[]
```

## Similarity Search

Find similar content:

```typescript
// Using db.search()
const similar = await db.Post.search('machine learning tutorials')

// Using embeddings directly
const queryEmbedding = await db.embed('machine learning')
const results = await db.embed.findSimilar(queryEmbedding, {
  type: 'Post',
  topK: 10,
  minScore: 0.7,
})
```

## Regenerating Embeddings

Force regeneration:

```typescript
// Delete existing
await db.artifact.delete(post.$id, 'embedding')

// Regenerate
const embedding = await db.embed(post.content)
await db.artifact.set(post.$id, 'embedding', {
  vectors: embedding,
  model: 'google/gemini-embedding-001',
  dimensions: 3072,
  sourceHash: hash(post.content),
})
```

## Batch Embedding

Embed many documents efficiently:

```typescript
const posts = await db.Post.list()

// Batch embed (more efficient than one-by-one)
const texts = posts.map(p => p.content)
const embeddings = await db.embed(texts)

// Store
for (let i = 0; i < posts.length; i++) {
  await db.artifact.set(posts[i].$id, 'embedding', {
    vectors: embeddings[i],
    model: 'google/gemini-embedding-001',
    sourceHash: hash(posts[i].content),
  })
}
```

## Chunking

Long content is chunked before embedding:

```typescript
// Long content is automatically chunked
const post = await db.Post.create({
  title: 'Comprehensive Guide',
  content: veryLongContent,  // 10,000 words
})

// Multiple chunk embeddings stored
const embedding = await db.artifact.get(post.$id, 'embedding')
embedding.chunks  // number of chunks
```

## Evaluating Models

Compare model performance:

```typescript
const models = [
  { model: 'google/gemini-embedding-001', dimensions: 3072 },
  { model: 'baai/bge-m3', dimensions: 1024 },
]

const testCases = [
  { query: 'how to start', expected: 'Getting Started' },
  { query: 'API parameters', expected: 'API Reference' },
]

for (const config of models) {
  const db = DB({ ...config, memory: true })

  // Seed test data
  for (const doc of testDocs) {
    await db.Doc.create(doc)
  }

  // Run evaluation
  let correct = 0
  for (const { query, expected } of testCases) {
    const [top] = await db.Doc.search(query, { limit: 1 })
    if (top?.title === expected) correct++
  }

  console.log(`${config.model}: ${correct}/${testCases.length}`)
}
```
